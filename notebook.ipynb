{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Basic RAG Application\n",
    "\n",
    "\n",
    "#### 1. Importing Libraries\n",
    "\n",
    "TextLoader helps load your text data from files.\n",
    "CharacterTextSplitter breaks large text into smaller chunks.\n",
    "OllamaEmbeddings converts text chunks into numerical vectors (embeddings).\n",
    "FAISS is a vector database that stores these embeddings and allows fast similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.vectorstores import FAISS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Load and Split Data\n",
    "\n",
    "<i>loader = TextLoader('data.txt')<i>\n",
    "This creates a loader to read the contents of a file called data.txt. This file should contain your source text or documents.\n",
    "\n",
    "<i>docs = loader.load()<i>\n",
    "This actually reads the file and loads the text into a variable called docs. Now docs holds the entire content as a document object.\n",
    "\n",
    "<i>text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=20)<i>\n",
    "Here, we create a splitter that will divide the big text into smaller pieces (chunks) of 100 characters each. The chunk_overlap=20 means that each chunk will share 20 characters with the previous chunk to keep context.\n",
    "\n",
    "<i>splits = text_splitter.split_documents(docs) <i>\n",
    "This applies the splitter to the loaded documents, producing a list of smaller text chunks stored in splits. These smaller chunks are easier to embed and search.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader('data.txt')\n",
    "docs = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "splits = text_splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Creating Embeddings and Vector Store\n",
    "\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"hf.co/CompendiumLabs/bge-base-en-v1.5-gguf\")\n",
    "This initializes an embedding model from Ollama. The model converts text chunks into vectors (arrays of numbers) that capture the meaning of the text. Here, we specify a particular embedding model hosted on Hugging Face.\n",
    "\n",
    "vectorstore = FAISS.from_documents(splits, embeddings)\n",
    "This line creates a FAISS vector store (database) by:\n",
    "\n",
    "Taking each chunk in splits.\n",
    "\n",
    "Converting it into an embedding using the embeddings model.\n",
    "\n",
    "Storing all these embeddings in FAISS for fast similarity search later.\n",
    "\n",
    "vectorstore.save_local(\"faiss_index\")\n",
    "This saves the built vector database locally on your machine (or Colab environment) in a folder named \"faiss_index\". This way, you can reuse the index without rebuilding it every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "vectorstore = FAISS.from_documents(splits, embeddings)\n",
    "vectorstore.save_local(\"faiss_index\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Setting up the Query Pipeline\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "Imports a special chain (pipeline) that combines retrieval and question answering.\n",
    "\n",
    "from langchain.llms import Ollama\n",
    "Imports the Ollama wrapper to access a language model for generation.\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "Converts the FAISS vector store into a retriever object.\n",
    "\n",
    "k=3 means when you ask a question, it will fetch the top 3 most relevant chunks from the vector store.\n",
    "\n",
    "llm = Ollama(model=\"hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF\")\n",
    "Initializes the language model (LLM) that will generate answers. Here, it uses a 1-billion parameter Llama 3 model fine-tuned for instructions, hosted on Hugging Face.\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(...)\n",
    "Creates a RetrievalQA chain that:\n",
    "\n",
    "Uses the retriever to find relevant documents.\n",
    "\n",
    "Passes those documents along with the user query to the llm.\n",
    "\n",
    "The chain_type=\"stuff\" means it concatenates all retrieved documents into a single prompt for the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import Ollama\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "llm = Ollama(model=\"llama2\")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Running a Query\n",
    "\n",
    "query = \"How many bones do cats have?\"\n",
    "This is the question you want to ask your RAG system.\n",
    "\n",
    "result = qa_chain({\"query\": query})\n",
    "This sends the query to the qa_chain. Internally, it:\n",
    "\n",
    "Converts the query to an embedding.\n",
    "\n",
    "Retrieves the top 3 relevant chunks from the vector database.\n",
    "\n",
    "Passes the query + retrieved chunks to the LLM.\n",
    "\n",
    "Gets the generated answer.\n",
    "\n",
    "print(result['result'])\n",
    "Prints the answer generated by the LLM based on the retrieved information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Tell me four facts about cats\"\n",
    "result = qa_chain({\"query\": query})\n",
    "print(result['result'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
